 data/ (optional)
This folder is for storing raw or preprocessed data locally.

Example files:

stock_data.csv

news_articles.json

processed_dataset.csv
Useful for:

Re-running the pipeline without fetching data again.

Keeping debug/training datasets.




 src/ â€” Source Code
Holds all your Python modules organized by task.

ğŸ“„ ingestion.py
Purpose: Collect data

ğŸ“¥ Gets stock price data using yfinance

ğŸ“° Fetches financial news articles using NewsAPI

Stores or returns this data as DataFrames





 preprocessing.py
Purpose: Process and combine datasets

ğŸ§  Performs sentiment analysis on news headlines using FinBERT or FinGPT

ğŸ“Š Aggregates daily sentiment scores

ğŸ”— Merges news + stock data into one feature set

ğŸ·ï¸ Adds target labels (whether price will rise next day)






 model.py
Purpose: Model training + MLflow logging

ğŸ“ˆ Trains a model (e.g. RandomForestClassifier or LSTM)

âœ… Splits into train/test sets

ğŸ“Š Evaluates using metrics like accuracy

ğŸ” Logs model and metrics to MLflow






 app.py
Purpose: FastAPI inference server

ğŸ”„ Loads the trained model using MLflow

ğŸŒ Hosts a REST API (on /predict)

Accepts JSON with features like:





pipeline.py (optional if using main.py)
Purpose: Wrap the entire ML pipeline as a single callable function.

Often used if you integrate with Prefect or Airflow






 main.py
Purpose: Your entry-point script to run the entire pipeline manually.

It typically does:

Call get_stock_data and get_news

Run analyze_sentiment and merge_with_stock

Prepare dataset

Train and log model with train_model







# ğŸ“ˆ Stock Sentiment Prediction with News + MLOps

This project predicts stock movement (rise/fall) using financial news sentiment and stock price data.

### ğŸ”§ Stack

- Python, FastAPI
- MLflow (for experiment tracking)
- Transformers (FinBERT)
- Docker
- GCP Cloud Run (deployment)
- Prefect (optional pipeline orchestration)

### ğŸš€ Run Locally

```bash
# Create venv + activate
python -m venv venv
.\venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt

# Train the model
python main.py

# Run API server
uvicorn src.app:app --reload
